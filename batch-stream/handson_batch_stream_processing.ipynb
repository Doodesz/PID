{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f61c3f",
   "metadata": {},
   "source": [
    "# Handson: Batch Processing vs Stream Processing\n",
    "## Mata Kuliah: Pemrosesan dan Infrastruktur Data\n",
    "\n",
    "**Tujuan Pembelajaran:**\n",
    "- Memahami konsep batch processing dan stream processing\n",
    "- Menguasai implementasi batch processing menggunakan Pandas dan Apache Spark\n",
    "- Memahami dan mengimplementasikan stream processing dengan Python dan Kafka\n",
    "- Membandingkan performa antara batch dan stream processing\n",
    "- Membangun pipeline data processing yang lengkap\n",
    "\n",
    "**Prerequisites:**\n",
    "- Python 3.8+\n",
    "- Jupyter Notebook\n",
    "- Pemahaman dasar tentang data processing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66e8bc8",
   "metadata": {},
   "source": [
    "## 1. Setup Environment dan Import Libraries\n",
    "\n",
    "Pertama-tama, kita akan menginstall dan mengimport semua library yang diperlukan untuk handson ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f675758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install pandas numpy matplotlib seaborn\n",
    "# !pip install pyspark\n",
    "# !pip install kafka-python\n",
    "# !pip install faker\n",
    "# !pip install psutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import threading\n",
    "import queue\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from faker import Faker\n",
    "import psutil\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090a12be",
   "metadata": {},
   "source": [
    "## 2. Batch Processing dengan Pandas\n",
    "\n",
    "Batch processing adalah metode pemrosesan data di mana data dikumpulkan dan diproses dalam batch atau kelompok pada interval waktu tertentu.\n",
    "\n",
    "### Karakteristik Batch Processing:\n",
    "- **High Throughput**: Dapat memproses volume data yang besar\n",
    "- **High Latency**: Delay antara data masuk dan hasil keluar\n",
    "- **Scheduled Processing**: Dijalankan pada waktu tertentu\n",
    "- **Complete Data**: Memproses dataset yang lengkap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1620f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample e-commerce data for batch processing\n",
    "fake = Faker()\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_ecommerce_data(num_records=100000):\n",
    "    \"\"\"Generate sample e-commerce transaction data\"\"\"\n",
    "    data = []\n",
    "    categories = ['Electronics', 'Clothing', 'Books', 'Home', 'Sports', 'Beauty']\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        record = {\n",
    "            'transaction_id': f'TXN_{i+1:06d}',\n",
    "            'customer_id': f'CUST_{random.randint(1, 10000):05d}',\n",
    "            'product_category': random.choice(categories),\n",
    "            'product_price': round(random.uniform(10, 1000), 2),\n",
    "            'quantity': random.randint(1, 5),\n",
    "            'timestamp': fake.date_time_between(start_date='-30d', end_date='now'),\n",
    "            'customer_age': random.randint(18, 70),\n",
    "            'customer_city': fake.city(),\n",
    "            'payment_method': random.choice(['Credit Card', 'Debit Card', 'PayPal', 'Cash'])\n",
    "        }\n",
    "        record['total_amount'] = round(record['product_price'] * record['quantity'], 2)\n",
    "        data.append(record)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate data\n",
    "print(\"üîÑ Generating e-commerce dataset...\")\n",
    "start_time = time.time()\n",
    "df = generate_ecommerce_data(100000)\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Dataset generated in {generation_time:.2f} seconds\")\n",
    "print(f\"üìä Dataset shape: {df.shape}\")\n",
    "print(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nüìù Sample data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df14423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Processing Operations dengan Pandas\n",
    "\n",
    "def batch_analytics(df):\n",
    "    \"\"\"Perform comprehensive batch analytics\"\"\"\n",
    "    print(\"üîç Starting Batch Analytics...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Sales by Category\n",
    "    category_sales = df.groupby('product_category').agg({\n",
    "        'total_amount': ['sum', 'mean', 'count'],\n",
    "        'quantity': 'sum'\n",
    "    }).round(2)\n",
    "    \n",
    "    # 2. Daily Sales Trend\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    daily_sales = df.groupby('date')['total_amount'].sum().reset_index()\n",
    "    \n",
    "    # 3. Customer Segmentation\n",
    "    customer_stats = df.groupby('customer_id').agg({\n",
    "        'total_amount': 'sum',\n",
    "        'transaction_id': 'count'\n",
    "    }).reset_index()\n",
    "    customer_stats.columns = ['customer_id', 'total_spent', 'transaction_count']\n",
    "    \n",
    "    # 4. Payment Method Analysis\n",
    "    payment_analysis = df.groupby('payment_method')['total_amount'].agg(['sum', 'count']).reset_index()\n",
    "    \n",
    "    # 5. Age Group Analysis\n",
    "    df['age_group'] = pd.cut(df['customer_age'], \n",
    "                           bins=[0, 25, 35, 45, 55, 100], \n",
    "                           labels=['18-25', '26-35', '36-45', '46-55', '56+'])\n",
    "    age_analysis = df.groupby('age_group')['total_amount'].agg(['sum', 'mean', 'count'])\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Batch processing completed in {processing_time:.2f} seconds\")\n",
    "    \n",
    "    return {\n",
    "        'category_sales': category_sales,\n",
    "        'daily_sales': daily_sales,\n",
    "        'customer_stats': customer_stats,\n",
    "        'payment_analysis': payment_analysis,\n",
    "        'age_analysis': age_analysis,\n",
    "        'processing_time': processing_time\n",
    "    }\n",
    "\n",
    "# Execute batch processing\n",
    "results = batch_analytics(df)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìä Category Sales Summary:\")\n",
    "print(results['category_sales'])\n",
    "\n",
    "print(f\"\\nüí∞ Total Revenue: ${df['total_amount'].sum():,.2f}\")\n",
    "print(f\"üõí Total Transactions: {len(df):,}\")\n",
    "print(f\"üë• Unique Customers: {df['customer_id'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a90405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisasi Hasil Batch Processing\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Sales by Category\n",
    "category_sales_plot = results['category_sales']['total_amount']['sum'].sort_values(ascending=True)\n",
    "axes[0, 0].barh(category_sales_plot.index, category_sales_plot.values)\n",
    "axes[0, 0].set_title('Total Sales by Category')\n",
    "axes[0, 0].set_xlabel('Total Sales ($)')\n",
    "\n",
    "# 2. Daily Sales Trend\n",
    "axes[0, 1].plot(results['daily_sales']['date'], results['daily_sales']['total_amount'])\n",
    "axes[0, 1].set_title('Daily Sales Trend')\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Sales ($)')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Payment Method Distribution\n",
    "payment_counts = results['payment_analysis']['count']\n",
    "axes[1, 0].pie(payment_counts, labels=results['payment_analysis']['payment_method'], autopct='%1.1f%%')\n",
    "axes[1, 0].set_title('Payment Method Distribution')\n",
    "\n",
    "# 4. Age Group Analysis\n",
    "age_sales = results['age_analysis']['sum']\n",
    "axes[1, 1].bar(age_sales.index, age_sales.values)\n",
    "axes[1, 1].set_title('Sales by Age Group')\n",
    "axes[1, 1].set_xlabel('Age Group')\n",
    "axes[1, 1].set_ylabel('Total Sales ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance metrics\n",
    "print(f\"\\n‚ö° Batch Processing Performance:\")\n",
    "print(f\"   - Processing Time: {results['processing_time']:.2f} seconds\")\n",
    "print(f\"   - Records Processed: {len(df):,}\")\n",
    "print(f\"   - Processing Rate: {len(df)/results['processing_time']:,.0f} records/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73336d4f",
   "metadata": {},
   "source": [
    "## 3. Batch Processing dengan Apache Spark (PySpark)\n",
    "\n",
    "Apache Spark adalah framework untuk processing data besar yang dapat menangani batch dan stream processing. Mari kita coba implementasi dengan PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8017db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark Setup with fallback to Pandas simulation\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, sum as spark_sum, avg, count, max as spark_max\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
    "    \n",
    "    # Initialize Spark Session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"BatchProcessingDemo\") \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Set log level to reduce verbose output\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    print(\"‚úÖ Spark Session created successfully!\")\n",
    "    print(f\"üî• Spark Version: {spark.version}\")\n",
    "    \n",
    "    # Convert Pandas DataFrame to Spark DataFrame\n",
    "    print(\"\\nüîÑ Converting Pandas DataFrame to Spark DataFrame...\")\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    \n",
    "    print(f\"üìä Spark DataFrame shape: {spark_df.count()} rows, {len(spark_df.columns)} columns\")\n",
    "    print(\"\\nüìù Spark DataFrame Schema:\")\n",
    "    spark_df.printSchema()\n",
    "    \n",
    "    SPARK_AVAILABLE = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è  PySpark not available or Java Runtime missing\")\n",
    "    print(f\"   Error: {type(e).__name__}\")\n",
    "    print(\"\\nüí° Alternatives untuk menjalankan Spark:\")\n",
    "    print(\"   1. Install Java: brew install openjdk@11\")\n",
    "    print(\"   2. Install PySpark: pip install pyspark\")\n",
    "    print(\"   3. Use Docker: docker run -it apache/spark:latest\")\n",
    "    print(\"   4. Use cloud services: Databricks, EMR, Google Dataproc\")\n",
    "    print(\"\\nüîÑ Continuing with Pandas simulation of Spark operations...\")\n",
    "    \n",
    "    SPARK_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c01b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark-style Operations (simulated with Pandas since Spark not available)\n",
    "if SPARK_AVAILABLE:\n",
    "    print(\"üîç Starting Spark Batch Analytics...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Sales by Category using Spark\n",
    "    spark_category_sales = spark_df.groupBy(\"product_category\") \\\n",
    "        .agg(spark_sum(\"total_amount\").alias(\"total_sales\"),\n",
    "             avg(\"total_amount\").alias(\"avg_sales\"),\n",
    "             count(\"*\").alias(\"transaction_count\")) \\\n",
    "        .orderBy(col(\"total_sales\").desc())\n",
    "    \n",
    "    print(\"üìä Category Sales (Spark):\")\n",
    "    spark_category_sales.show()\n",
    "    \n",
    "    # 2. Top customers\n",
    "    spark_top_customers = spark_df.groupBy(\"customer_id\") \\\n",
    "        .agg(spark_sum(\"total_amount\").alias(\"total_spent\"),\n",
    "             count(\"*\").alias(\"transactions\")) \\\n",
    "        .orderBy(col(\"total_spent\").desc()) \\\n",
    "        .limit(10)\n",
    "    \n",
    "    print(\"üèÜ Top 10 Customers (Spark):\")\n",
    "    spark_top_customers.show()\n",
    "    \n",
    "    # 3. Daily sales using Spark SQL\n",
    "    spark_df.createOrReplaceTempView(\"transactions\")\n",
    "    \n",
    "    daily_sales_spark = spark.sql(\"\"\"\n",
    "        SELECT DATE(timestamp) as date, \n",
    "               SUM(total_amount) as daily_total,\n",
    "               COUNT(*) as daily_transactions\n",
    "        FROM transactions \n",
    "        GROUP BY DATE(timestamp) \n",
    "        ORDER BY date\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"üìÖ Daily Sales (Spark SQL):\")\n",
    "    daily_sales_spark.show(10)\n",
    "    \n",
    "    spark_processing_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Spark processing completed in {spark_processing_time:.2f} seconds\")\n",
    "    \n",
    "    # Cache DataFrame for faster subsequent operations\n",
    "    spark_df.cache()\n",
    "    cached_count = spark_df.count()  # Trigger caching\n",
    "    print(f\"üöÄ DataFrame cached with {cached_count:,} records\")\n",
    "    \n",
    "else:\n",
    "    print(\"üîç Starting Spark-style Analytics with Pandas simulation...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Sales by Category (Spark-style with Pandas)\n",
    "    print(\"üìä Category Sales (Pandas simulation of Spark):\")\n",
    "    pandas_category_sales = df.groupby('product_category').agg({\n",
    "        'total_amount': ['sum', 'mean', 'count']\n",
    "    }).round(2)\n",
    "    pandas_category_sales.columns = ['total_sales', 'avg_sales', 'transaction_count']\n",
    "    pandas_category_sales = pandas_category_sales.sort_values('total_sales', ascending=False)\n",
    "    print(pandas_category_sales)\n",
    "    \n",
    "    # 2. Top customers (Spark-style with Pandas)\n",
    "    print(\"\\nüèÜ Top 10 Customers (Pandas simulation of Spark):\")\n",
    "    pandas_top_customers = df.groupby('customer_id').agg({\n",
    "        'total_amount': 'sum',\n",
    "        'transaction_id': 'count'\n",
    "    }).round(2)\n",
    "    pandas_top_customers.columns = ['total_spent', 'transactions']\n",
    "    pandas_top_customers = pandas_top_customers.sort_values('total_spent', ascending=False).head(10)\n",
    "    print(pandas_top_customers)\n",
    "    \n",
    "    # 3. Daily sales (SQL-style with Pandas)\n",
    "    print(\"\\nüìÖ Daily Sales (Pandas SQL-style):\")\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    pandas_daily_sales = df.groupby('date').agg({\n",
    "        'total_amount': 'sum',\n",
    "        'transaction_id': 'count'\n",
    "    }).round(2)\n",
    "    pandas_daily_sales.columns = ['daily_total', 'daily_transactions']\n",
    "    pandas_daily_sales = pandas_daily_sales.sort_index()\n",
    "    print(pandas_daily_sales.head(10))\n",
    "    \n",
    "    spark_processing_time = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Pandas simulation completed in {spark_processing_time:.2f} seconds\")\n",
    "    \n",
    "    # Memory usage instead of caching\n",
    "    memory_usage = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"üß† DataFrame memory usage: {memory_usage:.2f} MB\")\n",
    "    \n",
    "    print(f\"\\nüìà Performance Insights:\")\n",
    "    print(f\"   - Pandas is efficient for datasets < 1GB\")\n",
    "    print(f\"   - Spark excels with datasets > 1GB and distributed computing\")\n",
    "    print(f\"   - This simulation shows similar operations in both frameworks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbbdc9a",
   "metadata": {},
   "source": [
    "## 4. Stream Processing Simulation dengan Python\n",
    "\n",
    "Stream processing adalah metode pemrosesan data real-time di mana data diproses segera setelah tiba.\n",
    "\n",
    "### Karakteristik Stream Processing:\n",
    "- **Low Latency**: Minimal delay antara input dan output\n",
    "- **Continuous Processing**: Data diproses secara kontinyu\n",
    "- **Event-driven**: Respons terhadap event yang masuk\n",
    "- **Windowing**: Agregasi data dalam window waktu tertentu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75272db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict, deque\n",
    "from threading import Thread\n",
    "import queue\n",
    "\n",
    "class StreamProcessor:\n",
    "    \"\"\"Real-time stream processor simulation\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=60):  # 60 seconds window\n",
    "        self.window_size = window_size\n",
    "        self.data_stream = queue.Queue()\n",
    "        self.metrics = defaultdict(lambda: deque())\n",
    "        self.running = False\n",
    "        self.processed_count = 0\n",
    "        \n",
    "    def generate_stream_data(self, duration=30):\n",
    "        \"\"\"Generate streaming transaction data\"\"\"\n",
    "        print(f\"üåä Starting data stream for {duration} seconds...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while time.time() - start_time < duration:\n",
    "            # Generate random transaction\n",
    "            transaction = {\n",
    "                'timestamp': datetime.now(),\n",
    "                'transaction_id': f'STREAM_{self.processed_count + 1:06d}',\n",
    "                'customer_id': f'CUST_{random.randint(1, 1000):05d}',\n",
    "                'product_category': random.choice(['Electronics', 'Clothing', 'Books', 'Home']),\n",
    "                'amount': round(random.uniform(10, 500), 2),\n",
    "                'quantity': random.randint(1, 3)\n",
    "            }\n",
    "            \n",
    "            self.data_stream.put(transaction)\n",
    "            time.sleep(random.uniform(0.1, 0.5))  # Random intervals\n",
    "        \n",
    "        # Signal end of stream\n",
    "        self.data_stream.put(None)\n",
    "        print(\"üîö Stream generation completed\")\n",
    "    \n",
    "    def process_stream(self):\n",
    "        \"\"\"Process incoming stream data with windowing\"\"\"\n",
    "        print(\"üîÑ Starting stream processing...\")\n",
    "        \n",
    "        while self.running:\n",
    "            try:\n",
    "                # Get data from stream with timeout\n",
    "                data = self.data_stream.get(timeout=1)\n",
    "                \n",
    "                if data is None:  # End of stream signal\n",
    "                    break\n",
    "                \n",
    "                # Process the data\n",
    "                self.process_transaction(data)\n",
    "                self.processed_count += 1\n",
    "                \n",
    "                # Print real-time metrics every 10 transactions\n",
    "                if self.processed_count % 10 == 0:\n",
    "                    self.print_realtime_metrics()\n",
    "                    \n",
    "            except queue.Empty:\n",
    "                continue\n",
    "        \n",
    "        print(f\"‚úÖ Stream processing completed. Processed {self.processed_count} transactions\")\n",
    "    \n",
    "    def process_transaction(self, transaction):\n",
    "        \"\"\"Process individual transaction\"\"\"\n",
    "        current_time = transaction['timestamp']\n",
    "        \n",
    "        # Add to sliding window\n",
    "        category = transaction['product_category']\n",
    "        \n",
    "        # Store in time-based window\n",
    "        self.metrics[f\"{category}_amounts\"].append((current_time, transaction['amount']))\n",
    "        self.metrics[f\"{category}_count\"].append((current_time, 1))\n",
    "        \n",
    "        # Clean old data outside window\n",
    "        self.clean_old_data(current_time)\n",
    "    \n",
    "    def clean_old_data(self, current_time):\n",
    "        \"\"\"Remove data outside the time window\"\"\"\n",
    "        cutoff_time = current_time - timedelta(seconds=self.window_size)\n",
    "        \n",
    "        for key in list(self.metrics.keys()):\n",
    "            while (self.metrics[key] and \n",
    "                   self.metrics[key][0][0] < cutoff_time):\n",
    "                self.metrics[key].popleft()\n",
    "    \n",
    "    def print_realtime_metrics(self):\n",
    "        \"\"\"Print current window metrics\"\"\"\n",
    "        print(f\"\\nüìä Real-time Metrics (Last {self.window_size}s window):\")\n",
    "        print(f\"   Processed: {self.processed_count} transactions\")\n",
    "        \n",
    "        # Calculate metrics for each category\n",
    "        for category in ['Electronics', 'Clothing', 'Books', 'Home']:\n",
    "            amounts_key = f\"{category}_amounts\"\n",
    "            count_key = f\"{category}_count\"\n",
    "            \n",
    "            if amounts_key in self.metrics and self.metrics[amounts_key]:\n",
    "                total_amount = sum(amount for _, amount in self.metrics[amounts_key])\n",
    "                total_count = len(self.metrics[count_key])\n",
    "                avg_amount = total_amount / total_count if total_count > 0 else 0\n",
    "                \n",
    "                print(f\"   {category}: {total_count} txns, ${total_amount:.2f} total, ${avg_amount:.2f} avg\")\n",
    "\n",
    "# Initialize stream processor\n",
    "processor = StreamProcessor(window_size=30)\n",
    "\n",
    "# Start processing in a separate thread\n",
    "processor.running = True\n",
    "processing_thread = Thread(target=processor.process_stream)\n",
    "processing_thread.start()\n",
    "\n",
    "# Generate stream data\n",
    "stream_thread = Thread(target=processor.generate_stream_data, args=(20,))\n",
    "stream_thread.start()\n",
    "\n",
    "# Wait for completion\n",
    "stream_thread.join()\n",
    "processor.running = False\n",
    "processing_thread.join()\n",
    "\n",
    "print(f\"\\nüéØ Stream Processing Summary:\")\n",
    "print(f\"   Total Transactions Processed: {processor.processed_count}\")\n",
    "print(f\"   Average Processing Rate: {processor.processed_count/20:.1f} transactions/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420fe6af",
   "metadata": {},
   "source": [
    "## 5. Stream Processing dengan Kafka dan Python\n",
    "\n",
    "Apache Kafka adalah platform streaming terdistribusi yang memungkinkan real-time data streaming. Mari kita simulasikan penggunaan Kafka untuk stream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c88117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka Simulation (Mock implementation since actual Kafka requires server setup)\n",
    "class KafkaSimulator:\n",
    "    \"\"\"Simulate Kafka producer and consumer for learning purposes\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.topics = defaultdict(list)\n",
    "        self.consumers = {}\n",
    "        \n",
    "    def create_producer(self, topic):\n",
    "        \"\"\"Create a Kafka producer\"\"\"\n",
    "        return KafkaProducerSim(self, topic)\n",
    "        \n",
    "    def create_consumer(self, topic, group_id):\n",
    "        \"\"\"Create a Kafka consumer\"\"\"\n",
    "        return KafkaConsumerSim(self, topic, group_id)\n",
    "\n",
    "class KafkaProducerSim:\n",
    "    \"\"\"Simulated Kafka Producer\"\"\"\n",
    "    \n",
    "    def __init__(self, kafka_sim, topic):\n",
    "        self.kafka_sim = kafka_sim\n",
    "        self.topic = topic\n",
    "        self.message_count = 0\n",
    "        \n",
    "    def send(self, message):\n",
    "        \"\"\"Send message to topic\"\"\"\n",
    "        timestamp = datetime.now()\n",
    "        self.kafka_sim.topics[self.topic].append({\n",
    "            'timestamp': timestamp,\n",
    "            'message': message,\n",
    "            'offset': len(self.kafka_sim.topics[self.topic])\n",
    "        })\n",
    "        self.message_count += 1\n",
    "        \n",
    "    def flush(self):\n",
    "        \"\"\"Flush any pending messages\"\"\"\n",
    "        pass\n",
    "\n",
    "class KafkaConsumerSim:\n",
    "    \"\"\"Simulated Kafka Consumer\"\"\"\n",
    "    \n",
    "    def __init__(self, kafka_sim, topic, group_id):\n",
    "        self.kafka_sim = kafka_sim\n",
    "        self.topic = topic\n",
    "        self.group_id = group_id\n",
    "        self.offset = 0\n",
    "        \n",
    "    def poll(self, timeout_ms=1000):\n",
    "        \"\"\"Poll for new messages\"\"\"\n",
    "        messages = []\n",
    "        topic_messages = self.kafka_sim.topics[self.topic]\n",
    "        \n",
    "        while self.offset < len(topic_messages):\n",
    "            messages.append(topic_messages[self.offset])\n",
    "            self.offset += 1\n",
    "            \n",
    "        return {self.topic: messages} if messages else {}\n",
    "        \n",
    "    def commit(self):\n",
    "        \"\"\"Commit current offset\"\"\"\n",
    "        pass\n",
    "\n",
    "# Initialize Kafka simulator\n",
    "kafka_sim = KafkaSimulator()\n",
    "\n",
    "print(\"üöÄ Kafka Simulator initialized!\")\n",
    "print(\"üìù Note: This is a simulation. In production, you would:\")\n",
    "print(\"   1. Install kafka-python: pip install kafka-python\")\n",
    "print(\"   2. Start Kafka server\")\n",
    "print(\"   3. Create topics\")\n",
    "print(\"   4. Use actual KafkaProducer and KafkaConsumer classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea23cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Kafka-style streaming pipeline\n",
    "def kafka_producer_demo():\n",
    "    \"\"\"Demonstrate Kafka producer sending e-commerce events\"\"\"\n",
    "    print(\"üîÑ Starting Kafka Producer Demo...\")\n",
    "    \n",
    "    # Create producer for e-commerce events\n",
    "    producer = kafka_sim.create_producer('ecommerce-events')\n",
    "    \n",
    "    # Generate and send events\n",
    "    for i in range(50):\n",
    "        event = {\n",
    "            'event_id': f'EVENT_{i+1:03d}',\n",
    "            'event_type': random.choice(['purchase', 'cart_add', 'view', 'search']),\n",
    "            'customer_id': f'CUST_{random.randint(1, 100):03d}',\n",
    "            'product_id': f'PROD_{random.randint(1, 500):03d}',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'amount': round(random.uniform(10, 300), 2) if random.random() > 0.3 else None\n",
    "        }\n",
    "        \n",
    "        producer.send(json.dumps(event))\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"   üì§ Sent {i+1} events to topic 'ecommerce-events'\")\n",
    "            \n",
    "        time.sleep(0.1)  # Simulate real-time streaming\n",
    "    \n",
    "    producer.flush()\n",
    "    print(f\"‚úÖ Producer sent {producer.message_count} events\")\n",
    "    return producer.message_count\n",
    "\n",
    "def kafka_consumer_demo():\n",
    "    \"\"\"Demonstrate Kafka consumer processing events\"\"\"\n",
    "    print(\"\\nüîÑ Starting Kafka Consumer Demo...\")\n",
    "    \n",
    "    # Create consumer for processing events\n",
    "    consumer = kafka_sim.create_consumer('ecommerce-events', 'analytics-group')\n",
    "    \n",
    "    # Process events\n",
    "    processed_events = 0\n",
    "    event_types = defaultdict(int)\n",
    "    total_revenue = 0\n",
    "    \n",
    "    while True:\n",
    "        # Poll for new messages\n",
    "        message_batch = consumer.poll(timeout_ms=1000)\n",
    "        \n",
    "        if not message_batch:\n",
    "            break\n",
    "            \n",
    "        for topic, messages in message_batch.items():\n",
    "            for msg in messages:\n",
    "                event = json.loads(msg['message'])\n",
    "                processed_events += 1\n",
    "                \n",
    "                # Process different event types\n",
    "                event_types[event['event_type']] += 1\n",
    "                \n",
    "                if event['event_type'] == 'purchase' and event['amount']:\n",
    "                    total_revenue += event['amount']\n",
    "                \n",
    "                # Real-time processing simulation\n",
    "                if processed_events % 10 == 0:\n",
    "                    print(f\"   üì• Processed {processed_events} events\")\n",
    "    \n",
    "    consumer.commit()\n",
    "    \n",
    "    print(f\"‚úÖ Consumer processed {processed_events} events\")\n",
    "    print(f\"üìä Event Type Distribution:\")\n",
    "    for event_type, count in event_types.items():\n",
    "        print(f\"   {event_type}: {count}\")\n",
    "    print(f\"üí∞ Total Revenue from purchases: ${total_revenue:.2f}\")\n",
    "    \n",
    "    return processed_events, dict(event_types), total_revenue\n",
    "\n",
    "# Run Kafka demo\n",
    "producer_count = kafka_producer_demo()\n",
    "consumer_count, event_distribution, revenue = kafka_consumer_demo()\n",
    "\n",
    "# Verify message delivery\n",
    "print(f\"\\n‚úÖ Message Delivery Verification:\")\n",
    "print(f\"   Messages Sent: {producer_count}\")\n",
    "print(f\"   Messages Processed: {consumer_count}\")\n",
    "print(f\"   Delivery Success: {'‚úÖ' if producer_count == consumer_count else '‚ùå'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482fabc8",
   "metadata": {},
   "source": [
    "## 6. Real-time Data Processing Pipeline\n",
    "\n",
    "Mari kita bangun pipeline lengkap yang menggabungkan stream ingestion, real-time processing, dan output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94619477",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealTimePipeline:\n",
    "    \"\"\"Complete real-time data processing pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'total_events': 0,\n",
    "            'fraud_alerts': 0,\n",
    "            'high_value_transactions': 0,\n",
    "            'processing_latency': []\n",
    "        }\n",
    "        self.alerts = []\n",
    "        self.running = False\n",
    "        \n",
    "    def fraud_detection(self, transaction):\n",
    "        \"\"\"Simple fraud detection rules\"\"\"\n",
    "        # Rule 1: Unusually high amount\n",
    "        if transaction['amount'] > 1000:\n",
    "            return True, \"High amount transaction\"\n",
    "            \n",
    "        # Rule 2: Multiple transactions from same customer in short time\n",
    "        # (Simplified for demo)\n",
    "        if random.random() < 0.05:  # 5% chance for demo\n",
    "            return True, \"Suspicious pattern detected\"\n",
    "            \n",
    "        return False, None\n",
    "        \n",
    "    def process_transaction(self, transaction):\n",
    "        \"\"\"Process individual transaction with business logic\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 1. Fraud Detection\n",
    "        is_fraud, fraud_reason = self.fraud_detection(transaction)\n",
    "        \n",
    "        if is_fraud:\n",
    "            self.metrics['fraud_alerts'] += 1\n",
    "            alert = {\n",
    "                'timestamp': datetime.now(),\n",
    "                'transaction_id': transaction['transaction_id'],\n",
    "                'reason': fraud_reason,\n",
    "                'amount': transaction['amount']\n",
    "            }\n",
    "            self.alerts.append(alert)\n",
    "            print(f\"üö® FRAUD ALERT: {fraud_reason} - Transaction {transaction['transaction_id']}\")\n",
    "        \n",
    "        # 2. High-value transaction detection\n",
    "        if transaction['amount'] > 500:\n",
    "            self.metrics['high_value_transactions'] += 1\n",
    "            print(f\"üíé High-value transaction: ${transaction['amount']:.2f}\")\n",
    "        \n",
    "        # 3. Real-time analytics update\n",
    "        self.metrics['total_events'] += 1\n",
    "        \n",
    "        # 4. Calculate processing latency\n",
    "        processing_time = (time.time() - start_time) * 1000  # ms\n",
    "        self.metrics['processing_latency'].append(processing_time)\n",
    "        \n",
    "        return {\n",
    "            'transaction_id': transaction['transaction_id'],\n",
    "            'processed_at': datetime.now(),\n",
    "            'is_fraud': is_fraud,\n",
    "            'processing_time_ms': processing_time\n",
    "        }\n",
    "    \n",
    "    def run_pipeline(self, duration=15):\n",
    "        \"\"\"Run the complete pipeline\"\"\"\n",
    "        print(f\"üöÄ Starting Real-time Processing Pipeline for {duration} seconds...\")\n",
    "        self.running = True\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while time.time() - start_time < duration:\n",
    "            # Simulate incoming transaction\n",
    "            transaction = {\n",
    "                'transaction_id': f'RT_{self.metrics[\"total_events\"]+1:06d}',\n",
    "                'customer_id': f'CUST_{random.randint(1, 1000):05d}',\n",
    "                'amount': round(random.uniform(5, 1200), 2),\n",
    "                'timestamp': datetime.now(),\n",
    "                'merchant': random.choice(['Amazon', 'Walmart', 'Target', 'BestBuy', 'Costco'])\n",
    "            }\n",
    "            \n",
    "            # Process transaction\n",
    "            result = self.process_transaction(transaction)\n",
    "            \n",
    "            # Print metrics every 20 transactions\n",
    "            if self.metrics['total_events'] % 20 == 0:\n",
    "                self.print_metrics()\n",
    "            \n",
    "            # Simulate processing delay\n",
    "            time.sleep(random.uniform(0.1, 0.3))\n",
    "        \n",
    "        self.running = False\n",
    "        print(\"\\n‚úÖ Pipeline processing completed!\")\n",
    "        self.print_final_metrics()\n",
    "    \n",
    "    def print_metrics(self):\n",
    "        \"\"\"Print current metrics\"\"\"\n",
    "        avg_latency = np.mean(self.metrics['processing_latency']) if self.metrics['processing_latency'] else 0\n",
    "        print(f\"üìä Current Metrics: {self.metrics['total_events']} events, \"\n",
    "              f\"{self.metrics['fraud_alerts']} fraud alerts, \"\n",
    "              f\"{avg_latency:.2f}ms avg latency\")\n",
    "    \n",
    "    def print_final_metrics(self):\n",
    "        \"\"\"Print final pipeline metrics\"\"\"\n",
    "        avg_latency = np.mean(self.metrics['processing_latency'])\n",
    "        max_latency = np.max(self.metrics['processing_latency'])\n",
    "        min_latency = np.min(self.metrics['processing_latency'])\n",
    "        \n",
    "        print(f\"\\nüìà Final Pipeline Metrics:\")\n",
    "        print(f\"   Total Events Processed: {self.metrics['total_events']}\")\n",
    "        print(f\"   Fraud Alerts Generated: {self.metrics['fraud_alerts']}\")\n",
    "        print(f\"   High-Value Transactions: {self.metrics['high_value_transactions']}\")\n",
    "        print(f\"   Processing Latency:\")\n",
    "        print(f\"     - Average: {avg_latency:.2f}ms\")\n",
    "        print(f\"     - Min: {min_latency:.2f}ms\")\n",
    "        print(f\"     - Max: {max_latency:.2f}ms\")\n",
    "        print(f\"   Fraud Rate: {(self.metrics['fraud_alerts']/self.metrics['total_events']*100):.1f}%\")\n",
    "\n",
    "# Run the real-time pipeline\n",
    "pipeline = RealTimePipeline()\n",
    "pipeline.run_pipeline(duration=20)\n",
    "\n",
    "# Show fraud alerts\n",
    "if pipeline.alerts:\n",
    "    print(f\"\\nüö® Fraud Alerts Summary ({len(pipeline.alerts)} total):\")\n",
    "    for alert in pipeline.alerts[-5:]:  # Show last 5 alerts\n",
    "        print(f\"   {alert['timestamp'].strftime('%H:%M:%S')} - \"\n",
    "              f\"Transaction {alert['transaction_id']}: {alert['reason']} \"\n",
    "              f\"(${alert['amount']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6586de9",
   "metadata": {},
   "source": [
    "## 7. Performance Comparison: Batch vs Stream\n",
    "\n",
    "Mari kita bandingkan performa antara batch processing dan stream processing menggunakan dataset yang sama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c915ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_comparison():\n",
    "    \"\"\"Compare batch vs stream processing performance\"\"\"\n",
    "    \n",
    "    # Generate test dataset\n",
    "    test_size = 10000\n",
    "    print(f\"üß™ Performance Test - Processing {test_size:,} records\")\n",
    "    \n",
    "    # Generate test data\n",
    "    test_data = []\n",
    "    for i in range(test_size):\n",
    "        test_data.append({\n",
    "            'id': i,\n",
    "            'value': random.uniform(1, 1000),\n",
    "            'category': random.choice(['A', 'B', 'C', 'D']),\n",
    "            'timestamp': datetime.now() - timedelta(seconds=random.randint(0, 3600))\n",
    "        })\n",
    "    \n",
    "    test_df = pd.DataFrame(test_data)\n",
    "    \n",
    "    # Batch Processing Test\n",
    "    print(\"\\nüìä Batch Processing Test:\")\n",
    "    batch_start = time.time()\n",
    "    \n",
    "    # Batch operations\n",
    "    batch_results = {\n",
    "        'total_value': test_df['value'].sum(),\n",
    "        'avg_value': test_df['value'].mean(),\n",
    "        'category_counts': test_df['category'].value_counts().to_dict(),\n",
    "        'max_value': test_df['value'].max(),\n",
    "        'std_value': test_df['value'].std()\n",
    "    }\n",
    "    \n",
    "    batch_time = time.time() - batch_start\n",
    "    batch_memory = psutil.Process().memory_info().rss / 1024**2  # MB\n",
    "    \n",
    "    print(f\"   ‚è±Ô∏è  Processing Time: {batch_time:.4f} seconds\")\n",
    "    print(f\"   üß† Memory Usage: {batch_memory:.2f} MB\")\n",
    "    print(f\"   üöÄ Throughput: {test_size/batch_time:.0f} records/second\")\n",
    "    \n",
    "    # Stream Processing Test\n",
    "    print(\"\\nüåä Stream Processing Test:\")\n",
    "    stream_start = time.time()\n",
    "    \n",
    "    # Stream processing simulation\n",
    "    stream_results = {\n",
    "        'total_value': 0,\n",
    "        'count': 0,\n",
    "        'category_counts': defaultdict(int),\n",
    "        'max_value': 0,\n",
    "        'values_sum_sq': 0  # For std calculation\n",
    "    }\n",
    "    \n",
    "    latencies = []\n",
    "    \n",
    "    for record in test_data:\n",
    "        record_start = time.time()\n",
    "        \n",
    "        # Process individual record\n",
    "        stream_results['total_value'] += record['value']\n",
    "        stream_results['count'] += 1\n",
    "        stream_results['category_counts'][record['category']] += 1\n",
    "        stream_results['max_value'] = max(stream_results['max_value'], record['value'])\n",
    "        stream_results['values_sum_sq'] += record['value'] ** 2\n",
    "        \n",
    "        # Calculate per-record latency\n",
    "        record_latency = (time.time() - record_start) * 1000  # ms\n",
    "        latencies.append(record_latency)\n",
    "    \n",
    "    # Calculate final aggregations\n",
    "    stream_results['avg_value'] = stream_results['total_value'] / stream_results['count']\n",
    "    variance = (stream_results['values_sum_sq'] / stream_results['count']) - (stream_results['avg_value'] ** 2)\n",
    "    stream_results['std_value'] = variance ** 0.5\n",
    "    \n",
    "    stream_time = time.time() - stream_start\n",
    "    stream_memory = psutil.Process().memory_info().rss / 1024**2  # MB\n",
    "    \n",
    "    print(f\"   ‚è±Ô∏è  Processing Time: {stream_time:.4f} seconds\")\n",
    "    print(f\"   üß† Memory Usage: {stream_memory:.2f} MB\")\n",
    "    print(f\"   üöÄ Throughput: {test_size/stream_time:.0f} records/second\")\n",
    "    print(f\"   üìä Avg Record Latency: {np.mean(latencies):.4f}ms\")\n",
    "    print(f\"   üìä Max Record Latency: {np.max(latencies):.4f}ms\")\n",
    "    \n",
    "    # Comparison Summary\n",
    "    print(f\"\\n‚öñÔ∏è  Performance Comparison Summary:\")\n",
    "    print(f\"   Processing Time - Batch: {batch_time:.4f}s, Stream: {stream_time:.4f}s\")\n",
    "    print(f\"   Throughput - Batch: {test_size/batch_time:.0f} rec/s, Stream: {test_size/stream_time:.0f} rec/s\")\n",
    "    print(f\"   Memory Usage - Batch: {batch_memory:.1f}MB, Stream: {stream_memory:.1f}MB\")\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    print(f\"\\nüéØ Result Accuracy Comparison:\")\n",
    "    print(f\"   Total Value - Batch: {batch_results['total_value']:.2f}, Stream: {stream_results['total_value']:.2f}\")\n",
    "    print(f\"   Average Value - Batch: {batch_results['avg_value']:.2f}, Stream: {stream_results['avg_value']:.2f}\")\n",
    "    print(f\"   Max Value - Batch: {batch_results['max_value']:.2f}, Stream: {stream_results['max_value']:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'batch': {'time': batch_time, 'memory': batch_memory, 'results': batch_results},\n",
    "        'stream': {'time': stream_time, 'memory': stream_memory, 'results': stream_results, 'latencies': latencies}\n",
    "    }\n",
    "\n",
    "# Run performance comparison\n",
    "comparison_results = performance_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Performance Comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Processing Time Comparison\n",
    "methods = ['Batch', 'Stream']\n",
    "times = [comparison_results['batch']['time'], comparison_results['stream']['time']]\n",
    "axes[0, 0].bar(methods, times, color=['blue', 'green'])\n",
    "axes[0, 0].set_title('Processing Time Comparison')\n",
    "axes[0, 0].set_ylabel('Time (seconds)')\n",
    "\n",
    "# 2. Throughput Comparison\n",
    "throughputs = [10000/comparison_results['batch']['time'], 10000/comparison_results['stream']['time']]\n",
    "axes[0, 1].bar(methods, throughputs, color=['blue', 'green'])\n",
    "axes[0, 1].set_title('Throughput Comparison')\n",
    "axes[0, 1].set_ylabel('Records/Second')\n",
    "\n",
    "# 3. Memory Usage Comparison\n",
    "memory = [comparison_results['batch']['memory'], comparison_results['stream']['memory']]\n",
    "axes[1, 0].bar(methods, memory, color=['blue', 'green'])\n",
    "axes[1, 0].set_title('Memory Usage Comparison')\n",
    "axes[1, 0].set_ylabel('Memory (MB)')\n",
    "\n",
    "# 4. Stream Processing Latency Distribution\n",
    "latencies = comparison_results['stream']['latencies']\n",
    "axes[1, 1].hist(latencies, bins=50, color='green', alpha=0.7)\n",
    "axes[1, 1].set_title('Stream Processing Latency Distribution')\n",
    "axes[1, 1].set_xlabel('Latency (ms)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Processing Time (s)', 'Throughput (rec/s)', 'Memory Usage (MB)', 'Latency (ms)'],\n",
    "    'Batch Processing': [\n",
    "        f\"{comparison_results['batch']['time']:.4f}\",\n",
    "        f\"{10000/comparison_results['batch']['time']:.0f}\",\n",
    "        f\"{comparison_results['batch']['memory']:.2f}\",\n",
    "        'N/A (Batch)'\n",
    "    ],\n",
    "    'Stream Processing': [\n",
    "        f\"{comparison_results['stream']['time']:.4f}\",\n",
    "        f\"{10000/comparison_results['stream']['time']:.0f}\",\n",
    "        f\"{comparison_results['stream']['memory']:.2f}\",\n",
    "        f\"{np.mean(latencies):.4f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nüìã Performance Comparison Table:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e85ef0",
   "metadata": {},
   "source": [
    "## 8. Praktikum: Log Processing System\n",
    "\n",
    "Sebagai latihan akhir, mari kita implementasikan sistem pemrosesan log yang lengkap yang menangani batch analysis untuk log historis dan real-time monitoring untuk log yang masuk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d14081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class LogProcessor:\n",
    "    \"\"\"Complete log processing system for batch and stream processing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.log_pattern = re.compile(\n",
    "            r'(?P<ip>\\d+\\.\\d+\\.\\d+\\.\\d+) - - \\[(?P<timestamp>[^\\]]+)\\] '\n",
    "            r'\"(?P<method>\\w+) (?P<url>[^\\s]+) HTTP/[^\"]*\" '\n",
    "            r'(?P<status>\\d+) (?P<size>\\d+|-)'\n",
    "        )\n",
    "        \n",
    "    def generate_apache_logs(self, num_logs=5000):\n",
    "        \"\"\"Generate sample Apache access logs\"\"\"\n",
    "        methods = ['GET', 'POST', 'PUT', 'DELETE']\n",
    "        urls = ['/api/users', '/api/products', '/home', '/login', '/checkout', '/search']\n",
    "        status_codes = [200, 301, 404, 500, 503]\n",
    "        \n",
    "        logs = []\n",
    "        for i in range(num_logs):\n",
    "            ip = f\"{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}\"\n",
    "            timestamp = (datetime.now() - timedelta(hours=random.randint(0, 72))).strftime('%d/%b/%Y:%H:%M:%S +0000')\n",
    "            method = random.choice(methods)\n",
    "            url = random.choice(urls)\n",
    "            status = random.choice(status_codes)\n",
    "            size = random.randint(200, 5000) if status == 200 else '-'\n",
    "            \n",
    "            log_entry = f'{ip} - - [{timestamp}] \"{method} {url} HTTP/1.1\" {status} {size}'\n",
    "            logs.append(log_entry)\n",
    "            \n",
    "        return logs\n",
    "    \n",
    "    def parse_log_entry(self, log_entry):\n",
    "        \"\"\"Parse individual log entry\"\"\"\n",
    "        match = self.log_pattern.match(log_entry)\n",
    "        if match:\n",
    "            return match.groupdict()\n",
    "        return None\n",
    "    \n",
    "    def batch_log_analysis(self, logs):\n",
    "        \"\"\"Comprehensive batch analysis of historical logs\"\"\"\n",
    "        print(\"üîç Starting Batch Log Analysis...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        parsed_logs = []\n",
    "        parse_errors = 0\n",
    "        \n",
    "        # Parse all logs\n",
    "        for log in logs:\n",
    "            parsed = self.parse_log_entry(log)\n",
    "            if parsed:\n",
    "                parsed_logs.append(parsed)\n",
    "            else:\n",
    "                parse_errors += 1\n",
    "        \n",
    "        # Create DataFrame for analysis\n",
    "        df_logs = pd.DataFrame(parsed_logs)\n",
    "        df_logs['timestamp'] = pd.to_datetime(df_logs['timestamp'], format='%d/%b/%Y:%H:%M:%S +0000')\n",
    "        df_logs['status'] = df_logs['status'].astype(int)\n",
    "        df_logs['size'] = pd.to_numeric(df_logs['size'], errors='coerce')\n",
    "        \n",
    "        # Batch Analytics\n",
    "        analytics = {\n",
    "            'total_requests': len(df_logs),\n",
    "            'unique_ips': df_logs['ip'].nunique(),\n",
    "            'status_distribution': df_logs['status'].value_counts().to_dict(),\n",
    "            'method_distribution': df_logs['method'].value_counts().to_dict(),\n",
    "            'top_urls': df_logs['url'].value_counts().head(5).to_dict(),\n",
    "            'error_rate': (df_logs['status'] >= 400).sum() / len(df_logs) * 100,\n",
    "            'avg_response_size': df_logs['size'].mean(),\n",
    "            'peak_hour': df_logs['timestamp'].dt.hour.value_counts().index[0],\n",
    "            'parse_errors': parse_errors\n",
    "        }\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        analytics['processing_time'] = processing_time\n",
    "        \n",
    "        print(f\"‚úÖ Batch analysis completed in {processing_time:.2f} seconds\")\n",
    "        return analytics, df_logs\n",
    "    \n",
    "    def stream_log_monitoring(self, log_stream, duration=15):\n",
    "        \"\"\"Real-time log monitoring and alerting\"\"\"\n",
    "        print(f\"üåä Starting Real-time Log Monitoring for {duration} seconds...\")\n",
    "        \n",
    "        metrics = {\n",
    "            'total_requests': 0,\n",
    "            'error_count': 0,\n",
    "            'unique_ips': set(),\n",
    "            'suspicious_ips': Counter(),\n",
    "            'response_times': [],\n",
    "            'alerts': []\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        while time.time() - start_time < duration:\n",
    "            try:\n",
    "                # Simulate log entry arrival\n",
    "                if hasattr(log_stream, '__iter__'):\n",
    "                    log_entry = next(iter(log_stream))\n",
    "                else:\n",
    "                    log_entry = log_stream\n",
    "                \n",
    "                parsed = self.parse_log_entry(log_entry)\n",
    "                if not parsed:\n",
    "                    continue\n",
    "                \n",
    "                # Real-time processing\n",
    "                metrics['total_requests'] += 1\n",
    "                metrics['unique_ips'].add(parsed['ip'])\n",
    "                \n",
    "                # Track requests per IP for suspicious activity\n",
    "                metrics['suspicious_ips'][parsed['ip']] += 1\n",
    "                \n",
    "                # Error detection\n",
    "                status_code = int(parsed['status'])\n",
    "                if status_code >= 400:\n",
    "                    metrics['error_count'] += 1\n",
    "                    \n",
    "                    # Generate alert for 5xx errors\n",
    "                    if status_code >= 500:\n",
    "                        alert = {\n",
    "                            'timestamp': datetime.now(),\n",
    "                            'type': 'Server Error',\n",
    "                            'ip': parsed['ip'],\n",
    "                            'url': parsed['url'],\n",
    "                            'status': status_code\n",
    "                        }\n",
    "                        metrics['alerts'].append(alert)\n",
    "                        print(f\"üö® Server Error Alert: {parsed['ip']} - {parsed['url']} - Status {status_code}\")\n",
    "                \n",
    "                # Suspicious activity detection (>50 requests from same IP)\n",
    "                if metrics['suspicious_ips'][parsed['ip']] > 50:\n",
    "                    alert = {\n",
    "                        'timestamp': datetime.now(),\n",
    "                        'type': 'Suspicious Activity',\n",
    "                        'ip': parsed['ip'],\n",
    "                        'request_count': metrics['suspicious_ips'][parsed['ip']]\n",
    "                    }\n",
    "                    metrics['alerts'].append(alert)\n",
    "                    print(f\"üö® Suspicious Activity: {parsed['ip']} made {metrics['suspicious_ips'][parsed['ip']]} requests\")\n",
    "                \n",
    "                # Print metrics every 100 requests\n",
    "                if metrics['total_requests'] % 100 == 0:\n",
    "                    error_rate = (metrics['error_count'] / metrics['total_requests']) * 100\n",
    "                    print(f\"üìä Processed: {metrics['total_requests']} requests, \"\n",
    "                          f\"Error rate: {error_rate:.1f}%, \"\n",
    "                          f\"Unique IPs: {len(metrics['unique_ips'])}\")\n",
    "                \n",
    "                time.sleep(0.01)  # Simulate processing time\n",
    "                \n",
    "            except StopIteration:\n",
    "                break\n",
    "        \n",
    "        print(f\"‚úÖ Stream monitoring completed!\")\n",
    "        return metrics\n",
    "\n",
    "# Initialize log processor\n",
    "log_processor = LogProcessor()\n",
    "\n",
    "# Generate sample logs\n",
    "print(\"üìù Generating sample Apache access logs...\")\n",
    "sample_logs = log_processor.generate_apache_logs(5000)\n",
    "print(f\"Generated {len(sample_logs):,} log entries\")\n",
    "\n",
    "# Show sample log entries\n",
    "print(f\"\\nüìã Sample log entries:\")\n",
    "for i in range(3):\n",
    "    print(f\"   {sample_logs[i]}\")\n",
    "\n",
    "# Batch Analysis\n",
    "batch_results, logs_df = log_processor.batch_log_analysis(sample_logs)\n",
    "\n",
    "print(f\"\\nüìä Batch Analysis Results:\")\n",
    "print(f\"   Total Requests: {batch_results['total_requests']:,}\")\n",
    "print(f\"   Unique IPs: {batch_results['unique_ips']:,}\")\n",
    "print(f\"   Error Rate: {batch_results['error_rate']:.2f}%\")\n",
    "print(f\"   Peak Hour: {batch_results['peak_hour']}:00\")\n",
    "print(f\"   Processing Time: {batch_results['processing_time']:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nüîç Top 5 URLs:\")\n",
    "for url, count in batch_results['top_urls'].items():\n",
    "    print(f\"   {url}: {count} requests\")\n",
    "\n",
    "print(f\"\\nüìä Status Code Distribution:\")\n",
    "for status, count in sorted(batch_results['status_distribution'].items()):\n",
    "    print(f\"   {status}: {count} requests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82647fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream Monitoring Demo\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üåä STREAM MONITORING DEMO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Simulate real-time log stream\n",
    "def simulate_log_stream():\n",
    "    \"\"\"Generator that simulates real-time log entries\"\"\"\n",
    "    log_templates = [\n",
    "        '{ip} - - [{timestamp}] \"GET /api/users HTTP/1.1\" 200 1234',\n",
    "        '{ip} - - [{timestamp}] \"POST /login HTTP/1.1\" 200 567',\n",
    "        '{ip} - - [{timestamp}] \"GET /products HTTP/1.1\" 404 890',\n",
    "        '{ip} - - [{timestamp}] \"POST /checkout HTTP/1.1\" 500 0',\n",
    "        '{ip} - - [{timestamp}] \"GET /search HTTP/1.1\" 503 0'\n",
    "    ]\n",
    "    \n",
    "    while True:\n",
    "        ip = f\"{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}\"\n",
    "        timestamp = datetime.now().strftime('%d/%b/%Y:%H:%M:%S +0000')\n",
    "        template = random.choice(log_templates)\n",
    "        yield template.format(ip=ip, timestamp=timestamp)\n",
    "\n",
    "# Run stream monitoring\n",
    "log_stream = simulate_log_stream()\n",
    "stream_metrics = log_processor.stream_log_monitoring(log_stream, duration=10)\n",
    "\n",
    "print(f\"\\nüìà Stream Monitoring Results:\")\n",
    "print(f\"   Total Requests Processed: {stream_metrics['total_requests']:,}\")\n",
    "print(f\"   Error Count: {stream_metrics['error_count']}\")\n",
    "print(f\"   Unique IPs: {len(stream_metrics['unique_ips'])}\")\n",
    "print(f\"   Alerts Generated: {len(stream_metrics['alerts'])}\")\n",
    "\n",
    "if stream_metrics['alerts']:\n",
    "    print(f\"\\nüö® Recent Alerts:\")\n",
    "    for alert in stream_metrics['alerts'][-3:]:  # Show last 3 alerts\n",
    "        print(f\"   {alert['timestamp'].strftime('%H:%M:%S')} - {alert['type']}: {alert}\")\n",
    "\n",
    "# Visualize log analysis results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Status Code Distribution\n",
    "status_codes = list(batch_results['status_distribution'].keys())\n",
    "status_counts = list(batch_results['status_distribution'].values())\n",
    "axes[0, 0].bar(status_codes, status_counts, color=['green', 'orange', 'red'])\n",
    "axes[0, 0].set_title('HTTP Status Code Distribution')\n",
    "axes[0, 0].set_xlabel('Status Code')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "# 2. Request Methods\n",
    "methods = list(batch_results['method_distribution'].keys())\n",
    "method_counts = list(batch_results['method_distribution'].values())\n",
    "axes[0, 1].pie(method_counts, labels=methods, autopct='%1.1f%%')\n",
    "axes[0, 1].set_title('HTTP Method Distribution')\n",
    "\n",
    "# 3. Hourly Request Pattern\n",
    "hourly_requests = logs_df['timestamp'].dt.hour.value_counts().sort_index()\n",
    "axes[1, 0].plot(hourly_requests.index, hourly_requests.values, marker='o')\n",
    "axes[1, 0].set_title('Hourly Request Pattern')\n",
    "axes[1, 0].set_xlabel('Hour of Day')\n",
    "axes[1, 0].set_ylabel('Number of Requests')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# 4. Response Size Distribution\n",
    "valid_sizes = logs_df['size'].dropna()\n",
    "axes[1, 1].hist(valid_sizes, bins=30, alpha=0.7, color='skyblue')\n",
    "axes[1, 1].set_title('Response Size Distribution')\n",
    "axes[1, 1].set_xlabel('Response Size (bytes)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Log Processing System Summary:\")\n",
    "print(f\"   Batch Processing: Analyzed {batch_results['total_requests']:,} historical logs\")\n",
    "print(f\"   Stream Processing: Monitored {stream_metrics['total_requests']:,} real-time logs\")\n",
    "print(f\"   System successfully demonstrated both processing paradigms!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff58f740",
   "metadata": {},
   "source": [
    "## üìö Kesimpulan dan Pembelajaran\n",
    "\n",
    "### Perbedaan Utama Batch vs Stream Processing:\n",
    "\n",
    "| Aspek | Batch Processing | Stream Processing |\n",
    "|-------|------------------|-------------------|\n",
    "| **Latency** | High (minutes-hours) | Low (milliseconds-seconds) |\n",
    "| **Throughput** | Very High | Medium-High |\n",
    "| **Data Volume** | Large datasets | Individual records/small batches |\n",
    "| **Use Cases** | Historical analysis, ETL, Reports | Real-time monitoring, Fraud detection |\n",
    "| **Complexity** | Lower | Higher |\n",
    "| **Resource Usage** | Scheduled, intensive | Continuous, moderate |\n",
    "\n",
    "### Kapan Menggunakan Batch Processing:\n",
    "- ‚úÖ Analisis data historis\n",
    "- ‚úÖ ETL (Extract, Transform, Load) pipelines\n",
    "- ‚úÖ Laporan berkala (harian, mingguan, bulanan)\n",
    "- ‚úÖ Machine learning model training\n",
    "- ‚úÖ Data warehousing operations\n",
    "\n",
    "### Kapan Menggunakan Stream Processing:\n",
    "- ‚úÖ Real-time monitoring dan alerting\n",
    "- ‚úÖ Fraud detection\n",
    "- ‚úÖ Live dashboards\n",
    "- ‚úÖ IoT data processing\n",
    "- ‚úÖ Recommendation engines\n",
    "- ‚úÖ Trading systems\n",
    "\n",
    "### Tools dan Teknologi:\n",
    "\n",
    "**Batch Processing:**\n",
    "- Apache Spark\n",
    "- Apache Hadoop (MapReduce)\n",
    "- Pandas (Python)\n",
    "- Apache Airflow\n",
    "- AWS Batch, Google Cloud Dataflow\n",
    "\n",
    "**Stream Processing:**\n",
    "- Apache Kafka + Kafka Streams\n",
    "- Apache Storm\n",
    "- Apache Flink\n",
    "- Apache Spark Streaming\n",
    "- AWS Kinesis, Azure Stream Analytics\n",
    "\n",
    "### Hybrid Architecture:\n",
    "Dalam praktik modern, banyak sistem menggunakan **Lambda Architecture** atau **Kappa Architecture** yang menggabungkan batch dan stream processing untuk mendapatkan keuntungan dari kedua paradigma.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabcce6e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìù PENUGASAN: Analisis dan Pembahasan Kode\n",
    "\n",
    "## Petunjuk Pengerjaan:\n",
    "1. **Jawab semua pertanyaan dengan detail dan contoh kode**\n",
    "2. **Sertakan screenshot hasil eksekusi jika diperlukan**\n",
    "3. **Berikan analisis perbandingan yang mendalam**\n",
    "4. **Tugas dikerjakan secara individual**\n",
    "5. **Format jawaban dalam markdown atau dokumen terstruktur**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafb0c61",
   "metadata": {},
   "source": [
    "## üéØ TUGAS 1: Analisis Batch Processing dengan Pandas\n",
    "\n",
    "### A. Analisis Kode Generate Data\n",
    "\n",
    "**Pertanyaan:**\n",
    "1. Jelaskan mengapa kita menggunakan `np.random.seed(42)` dalam fungsi `generate_ecommerce_data()`?\n",
    "2. Analisis struktur data yang dihasilkan dan jelaskan setiap field yang ada\n",
    "3. Berapa memory usage untuk dataset 100,000 records? Hitung estimasi untuk 1 juta records\n",
    "4. Mengapa kita menggunakan `round()` pada `product_price` dan `total_amount`?\n",
    "\n",
    "**Yang harus dijawab:**\n",
    "```python\n",
    "# Contoh analisis yang diharapkan:\n",
    "# 1. Fungsi random seed\n",
    "# 2. Analisis memory usage\n",
    "# 3. Data type optimization\n",
    "# 4. Scaling considerations\n",
    "```\n",
    "\n",
    "### B. Analisis Operasi Batch Processing\n",
    "\n",
    "**Pertanyaan:**\n",
    "1. Jelaskan setiap operasi dalam fungsi `batch_analytics()`:\n",
    "   - Category sales aggregation\n",
    "   - Daily sales trend\n",
    "   - Customer segmentation\n",
    "   - Payment method analysis\n",
    "   - Age group analysis\n",
    "\n",
    "2. Mengapa kita menggunakan `.round(2)` pada hasil aggregation?\n",
    "\n",
    "3. Analisis performa: bagaimana performa berubah jika dataset bertambah 10x lipat?\n",
    "\n",
    "4. Bandingkan penggunaan `groupby()` vs `pivot_table()` untuk kasus ini\n",
    "\n",
    "**Kode yang harus dianalisis:**\n",
    "```python\n",
    "# Analisis kode berikut:\n",
    "category_sales = df.groupby('product_category').agg({\n",
    "    'total_amount': ['sum', 'mean', 'count'],\n",
    "    'quantity': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "# Bandingkan dengan alternatif:\n",
    "# pivot_table, query(), dll\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ded67ae",
   "metadata": {},
   "source": [
    "## üéØ TUGAS 2: Analisis Spark vs Pandas Simulation\n",
    "\n",
    "### A. Perbandingan Implementasi\n",
    "\n",
    "**Pertanyaan:**\n",
    "1. Bandingkan syntax Spark SQL vs Pandas operations. Berikan 3 contoh operasi yang menunjukkan perbedaan:\n",
    "\n",
    "```sql\n",
    "-- Spark SQL\n",
    "SELECT DATE(timestamp) as date, \n",
    "       SUM(total_amount) as daily_total,\n",
    "       COUNT(*) as daily_transactions\n",
    "FROM transactions \n",
    "GROUP BY DATE(timestamp) \n",
    "ORDER BY date\n",
    "```\n",
    "\n",
    "```python\n",
    "# Pandas equivalent\n",
    "df.groupby(df['timestamp'].dt.date).agg({\n",
    "    'total_amount': 'sum',\n",
    "    'transaction_id': 'count'\n",
    "})\n",
    "```\n",
    "\n",
    "2. Mengapa Spark menggunakan lazy evaluation? Apa keuntungan dan kerugiannya?\n",
    "\n",
    "3. Jelaskan konsep caching dalam Spark dan bandingkan dengan memory management Pandas\n",
    "\n",
    "4. Kapan sebaiknya menggunakan Spark vs Pandas? Berikan decision matrix\n",
    "\n",
    "### B. Error Handling dan Fallback Strategy\n",
    "\n",
    "**Pertanyaan:**\n",
    "1. Analisis error handling dalam kode PySpark setup:\n",
    "```python\n",
    "try:\n",
    "    spark = SparkSession.builder...\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è PySpark not available...\")\n",
    "    SPARK_AVAILABLE = False\n",
    "```\n",
    "\n",
    "2. Jelaskan mengapa kita perlu fallback ke Pandas simulation\n",
    "\n",
    "3. Berikan 3 alternatif deployment untuk Spark yang disebutkan dalam error message\n",
    "\n",
    "4. Apa dampak performance jika kita menggunakan Pandas untuk big data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c967c32",
   "metadata": {},
   "source": [
    "## üéØ TUGAS 3: Analisis Stream Processing Implementation\n",
    "\n",
    "### A. Stream Processor Architecture\n",
    "\n",
    "\n",
    "**Pertanyaan:**\n",
    "1. Analisis class `StreamProcessor` dan jelaskan setiap komponen:\n",
    "   - `data_stream` (Queue)\n",
    "   - `metrics` (defaultdict dengan deque)\n",
    "   - `window_size` parameter\n",
    "   - Threading implementation\n",
    "\n",
    "2. Jelaskan konsep sliding window dalam stream processing:\n",
    "```python\n",
    "def clean_old_data(self, current_time):\n",
    "    cutoff_time = current_time - timedelta(seconds=self.window_size)\n",
    "    for key in list(self.metrics.keys()):\n",
    "        while (self.metrics[key] and \n",
    "               self.metrics[key][0][0] < cutoff_time):\n",
    "            self.metrics[key].popleft()\n",
    "```\n",
    "\n",
    "3. Mengapa menggunakan `deque()` instead of regular list untuk windowing?\n",
    "\n",
    "4. Analisis trade-off antara window size dan memory usage\n",
    "\n",
    "### B. Threading dan Concurrency\n",
    "\n",
    "**Pertanyaan:**\n",
    "1. Jelaskan mengapa kita menggunakan 2 thread terpisah:\n",
    "   - `processing_thread` \n",
    "   - `stream_thread`\n",
    "\n",
    "2. Bagaimana koordinasi antara producer dan consumer thread?\n",
    "\n",
    "3. Apa yang terjadi jika processing thread lebih lambat dari data generation?\n",
    "\n",
    "4. Analisis potential race conditions dan bagaimana mengatasinya\n",
    "\n",
    "**Kode yang harus dianalisis:**\n",
    "```python\n",
    "# Thread coordination\n",
    "processor.running = True\n",
    "processing_thread = Thread(target=processor.process_stream)\n",
    "processing_thread.start()\n",
    "\n",
    "stream_thread = Thread(target=processor.generate_stream_data, args=(20,))\n",
    "stream_thread.start()\n",
    "\n",
    "# Wait for completion\n",
    "stream_thread.join()\n",
    "processor.running = False\n",
    "processing_thread.join()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
